{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Base code : https://yamalab.tistory.com/92","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm_notebook as tqdm\n\nimport numpy as np\n\n# Base code : https://yamalab.tistory.com/92\nclass MatrixFactorization():\n    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n        \"\"\"\n        :param R: rating matrix\n        :param k: latent parameter\n        :param learning_rate: alpha on weight update\n        :param reg_param: beta on weight update\n        :param epochs: training epochs\n        :param verbose: print status\n        \"\"\"\n        self._R = R\n        self._num_users, self._num_items = R.shape # 시용자와 아이템의 수\n        self._k = k # latent space의 크기 직접 사용자가 설정\n        self._learning_rate = learning_rate # 학습률\n        self._reg_param = reg_param # 규제를 얼마나 줄지\n        self._epochs = epochs # 반복 수\n        self._verbose = verbose","metadata":{"execution":{"iopub.status.busy":"2023-09-03T11:58:40.293107Z","iopub.execute_input":"2023-09-03T11:58:40.293648Z","iopub.status.idle":"2023-09-03T11:58:40.306401Z","shell.execute_reply.started":"2023-09-03T11:58:40.293600Z","shell.execute_reply":"2023-09-03T11:58:40.304741Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"    def fit(self):\n        \"\"\"\n        training Matrix Factorization : Update matrix latent weight and bias\n\n        참고: self._b에 대한 설명\n        - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n        - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n\n        :return: training_process\n        \"\"\"\n\n        # init latent features\n        self._P = np.random.normal(size=(self._num_users, self._k)) # 유저 latent space\n        self._Q = np.random.normal(size=(self._num_items, self._k)) # 아이템 latent space\n        # 정규분포를 띄도록 랜덤하게 latent space 생성\n        \n        # init biases\n        self._b_P = np.zeros(self._num_users)\n        self._b_Q = np.zeros(self._num_items)\n        self._b = np.mean(self._R[np.where(self._R != 0)])\n\n        # train while epochs\n        self._training_process = []\n        for epoch in range(self._epochs):\n            # rating이 존재하는 index를 기준으로 training\n            xi, yi = self._R.nonzero() # 평점이 있는 부분, 0이 아닌 부분의 x, y만 추출\n            for i, j in zip(xi, yi):\n                self.gradient_descent(i, j, self._R[i, j])\n            cost = self.cost()\n            self._training_process.append((epoch, cost))\n\n            # print status\n            if self._verbose == True and ((epoch + 1) % 10 == 0):\n                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))","metadata":{"execution":{"iopub.status.busy":"2023-09-03T12:06:21.522594Z","iopub.execute_input":"2023-09-03T12:06:21.523040Z","iopub.status.idle":"2023-09-03T12:06:21.539843Z","shell.execute_reply.started":"2023-09-03T12:06:21.523002Z","shell.execute_reply":"2023-09-03T12:06:21.538257Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"    def cost(self):\n        \"\"\"\n        compute root mean square error\n        :return: rmse cost\n        \"\"\"\n\n        # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n        # 참고: http://codepractice.tistory.com/90\n        xi, yi = self._R.nonzero()\n        # predicted = self.get_complete_matrix()\n        cost = 0\n        for x, y in zip(xi, yi):\n            cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n        return np.sqrt(cost/len(xi))\n\n\n    def gradient(self, error, i, j):\n        \"\"\"\n        gradient of latent feature for GD\n\n        :param error: rating - prediction error\n        :param i: user index\n        :param j: item index\n        :return: gradient of latent feature tuple\n        \"\"\"\n\n        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n        return dp, dq\n\n\n    def gradient_descent(self, i, j, rating):\n        \"\"\"\n        graident descent function\n\n        :param i: user index of matrix\n        :param j: item index of matrix\n        :param rating: rating of (i,j)\n        \"\"\"\n\n        # get error\n        prediction = self.get_prediction(i, j)\n        error = rating - prediction\n\n        # update biases\n        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n        # update latent feature\n        dp, dq = self.gradient(error, i, j)\n        self._P[i, :] += self._learning_rate * dp\n        self._Q[j, :] += self._learning_rate * dq\n\n\n    def get_prediction(self, i, j): # 유저 matrix와 아이템 matrix를 곱해서 예측한 행렬을 만들어주는 함수\n        \"\"\"\n        get predicted rating: user_i, item_j\n        :return: prediction of r_ij\n        \"\"\"\n        # 평점에 대한 bias조정은 안되어 있음\n        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n        \n\n    def get_complete_matrix(self):\n        \"\"\"\n        computer complete matrix PXQ + P.bias + Q.bias + global bias\n\n        - PXQ 행렬에 b_P[:, np.newaxis]를 더하는 것은 각 열마다 bias를 더해주는 것\n        - b_Q[np.newaxis:, ]를 더하는 것은 각 행마다 bias를 더해주는 것\n        - b를 더하는 것은 각 element마다 bias를 더해주는 것\n\n        - newaxis: 차원을 추가해줌. 1차원인 Latent들로 2차원의 R에 행/열 단위 연산을 해주기위해 차원을 추가하는 것.\n\n        :return: complete matrix R^\n        \"\"\"\n        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)","metadata":{"execution":{"iopub.status.busy":"2023-09-03T12:06:22.039037Z","iopub.execute_input":"2023-09-03T12:06:22.039522Z","iopub.status.idle":"2023-09-03T12:06:22.066541Z","shell.execute_reply.started":"2023-09-03T12:06:22.039482Z","shell.execute_reply":"2023-09-03T12:06:22.064855Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# run example\nif __name__ == \"__main__\":\n    # rating matrix - User X Item : (7 X 5)\n    R = np.array([\n        [1, 0, 0, 1, 3],\n        [2, 0, 3, 1, 1],\n        [1, 2, 0, 5, 0],\n        [1, 0, 0, 4, 4],\n        [2, 1, 5, 4, 0],\n        [5, 1, 5, 4, 0],\n        [0, 0, 0, 1, 0],\n    ])\n\n    # P, Q is (7 X k), (k X 5) matrix","metadata":{"execution":{"iopub.status.busy":"2023-09-03T12:06:22.683959Z","iopub.execute_input":"2023-09-03T12:06:22.684421Z","iopub.status.idle":"2023-09-03T12:06:22.694733Z","shell.execute_reply.started":"2023-09-03T12:06:22.684384Z","shell.execute_reply":"2023-09-03T12:06:22.693287Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time\nfactorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\nfactorizer.fit()","metadata":{"execution":{"iopub.status.busy":"2023-09-03T12:06:23.094444Z","iopub.execute_input":"2023-09-03T12:06:23.094915Z","iopub.status.idle":"2023-09-03T12:06:23.119745Z","shell.execute_reply.started":"2023-09-03T12:06:23.094878Z","shell.execute_reply":"2023-09-03T12:06:23.117774Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'MatrixFactorization' object has no attribute 'fit'"],"ename":"AttributeError","evalue":"'MatrixFactorization' object has no attribute 'fit'","output_type":"error"}]},{"cell_type":"code","source":"factorizer.get_complete_matrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 코드의 Step by Step 설명","metadata":{}},{"cell_type":"markdown","source":"기본적인 구조는 위와 같습니다. 한번 __init__ 부터 차근차근 코드의 설명을 시작하겠습니다.","metadata":{}},{"cell_type":"code","source":"class MatrixFactorization():\n    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n        \"\"\"\n        :param R: rating matrix\n        :param k: latent parameter\n        :param learning_rate: alpha on weight update\n        :param reg_param: beta on weight update\n        :param epochs: training epochs\n        :param verbose: print status\n        \"\"\"\n        self._R = R\n        self._num_users, self._num_items = R.shape\n        self._k = k\n        self._learning_rate = learning_rate\n        self._reg_param = reg_param\n        self._epochs = epochs\n        self._verbose = verbose","metadata":{"execution":{"iopub.status.busy":"2023-09-03T12:04:27.448364Z","iopub.execute_input":"2023-09-03T12:04:27.449136Z","iopub.status.idle":"2023-09-03T12:04:27.458505Z","shell.execute_reply.started":"2023-09-03T12:04:27.449054Z","shell.execute_reply":"2023-09-03T12:04:27.456726Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"일단, __init__ 함수는 Matrix Factorization이라는 클래스가 호출될 때 자동으로 실행되는 부분입니다. 파라미터로는 6개의 인자를 받는데 각각 아래의 의미를 가집니다.\n\n- R: 평점 행렬\n- k: User Latent와 Item Latent의 차원의 수\n- learning_rate: 학습률\n- reg_param: Weight의 Regularization 값\n- epochs: 전체 학습 횟수 (Total Epoch)\n- verbose: 학습 과정을 출력할지 여부 (True : 10번마다 cost 출력, False : cost를 출력하지 않음)","metadata":{}},{"cell_type":"code","source":"%%time\n\nR = np.array([\n    [1, 0, 0, 1, 3],\n    [2, 0, 3, 1, 1],\n    [1, 2, 0, 5, 0],\n    [1, 0, 0, 4, 4],\n    [2, 1, 5, 4, 0],\n    [5, 1, 5, 4, 0],\n    [0, 0, 0, 1, 0],\n])\n\nfactorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\nfactorizer.fit()","metadata":{"execution":{"iopub.status.busy":"2023-09-03T12:05:24.733353Z","iopub.execute_input":"2023-09-03T12:05:24.734108Z","iopub.status.idle":"2023-09-03T12:05:24.753588Z","shell.execute_reply.started":"2023-09-03T12:05:24.734032Z","shell.execute_reply":"2023-09-03T12:05:24.752410Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'MatrixFactorization' object has no attribute 'fit'"],"ename":"AttributeError","evalue":"'MatrixFactorization' object has no attribute 'fit'","output_type":"error"}]},{"cell_type":"markdown","source":"위의 실행은 7개의 User, 5개의 Item에 대한 평점 행렬(R)에 대해서 k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True 와 같은 파라미터를 가지는 MatrixFactorization 객체를 생성하라는 의미입니다. 생성된 객체에서 factorizer.fit()을 통해서 fit() 함수를 실행하면, 아래의 코드가 실행되게 됩니다.","metadata":{}},{"cell_type":"code","source":"def fit(self):\n    \"\"\"\n    training Matrix Factorization : Update matrix latent weight and bias\n\n    참고: self._b에 대한 설명\n    - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n    - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n\n    :return: training_process\n    \"\"\"\n\n    # init latent features\n    self._P = np.random.normal(size=(self._num_users, self._k)) # 유저 latent space\n    self._Q = np.random.normal(size=(self._num_items, self._k)) # 아이템 latent space\n    # 정규분포를 띄도록 랜덤하게 latent space 생성\n\n    # init biases\n    self._b_P = np.zeros(self._num_users)\n    self._b_Q = np.zeros(self._num_items)\n    self._b = np.mean(self._R[np.where(self._R != 0)])\n\n    # train while epochs\n    self._training_process = []\n    for epoch in range(self._epochs):\n        # rating이 존재하는 index를 기준으로 training\n        xi, yi = self._R.nonzero() # 평점이 있는 부분, 0이 아닌 부분의 x, y만 추출\n        for i, j in zip(xi, yi):\n            self.gradient_descent(i, j, self._R[i, j])\n        cost = self.cost()\n        self._training_process.append((epoch, cost))\n\n        # print status\n        if self._verbose == True and ((epoch + 1) % 10 == 0):\n            print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))","metadata":{"execution":{"iopub.status.busy":"2023-09-03T12:05:22.948155Z","iopub.execute_input":"2023-09-03T12:05:22.948653Z","iopub.status.idle":"2023-09-03T12:05:22.966085Z","shell.execute_reply.started":"2023-09-03T12:05:22.948614Z","shell.execute_reply":"2023-09-03T12:05:22.964456Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"fit 함수에서 가장 먼저 실행되는 부분은 아래의 Latent Matrix를 초기화해주는 부분입니다.","metadata":{}},{"cell_type":"code","source":"# init latent features\nself._P = np.random.normal(size=(self._num_users, self._k))\nself._Q = np.random.normal(size=(self._num_items, self._k))\n\n# init biases\nself._b_P = np.zeros(self._num_users)\nself._b_Q = np.zeros(self._num_items)\nself._b = np.mean(self._R[np.where(self._R != 0)])","metadata":{"execution":{"iopub.status.busy":"2023-09-03T12:05:27.740437Z","iopub.execute_input":"2023-09-03T12:05:27.740923Z","iopub.status.idle":"2023-09-03T12:05:27.769978Z","shell.execute_reply.started":"2023-09-03T12:05:27.740862Z","shell.execute_reply":"2023-09-03T12:05:27.768502Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-09d6ed2d0841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# init latent features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_P\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# init biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"],"ename":"NameError","evalue":"name 'self' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"이때, np.random.normal 함수를 이용해서 행렬을 초기화해주고 np.zeros 함수를 이용해서 bias 부분을 초기화해줍니다. 함수의 의미는 아래와 같습니다.\n\n- np.random.normal : (self._num_users, self._k)의 크기로 행렬을 정규분포 형태로 초기화합니다. 위의 예시에서는 User Latent Matrix는 (7, 3)의 크기를 Item Latent Matrix는 (5, 3)의 크기를 가짐\n- np.zeros : self._num_users 혹은 self._num_items의 크기만큼의 0 값을 가지는 벡터를 생성합니다.\n- np.mean(self._R[np.where(self._R != 0)]) : 전체 평점의 평균을 계산\n\n이후, 전체 학습 과정을 진행합니다.","metadata":{}},{"cell_type":"code","source":"# train while epochs\nself._training_process = []\nfor epoch in range(self._epochs):\n    # rating이 존재하는 index를 기준으로 training\n    xi, yi = self._R.nonzero()\n    for i, j in zip(xi, yi):\n        self.gradient_descent(i, j, self._R[i, j])\n    cost = self.cost()\n    self._training_process.append((epoch, cost))\n\n    # print status\n    if self._verbose == True and ((epoch + 1) % 10 == 0):\n        print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- self._training_process = [] 는 for문 안의 self._training_process.append((epoch, cost)) 에 사용되는 부분으로 학습 시에 Epoch와 Cost를 저장하는 부분입니다. for문의 경우 처음 파라미터로 받은 self._epochs 만큼 반복학습을 진행하게 됩니다.\n\n\n- 먼저 시행되는 for문의 첫 번째로 시행되는 xi, yi = self._R.nonzero()는 평점 행렬에서 0이 아닌 부분 즉, 사용자가 평점을 매긴 부분에 대해서만 값을 추출하라는 의미입니다. 그 이유는 위의 이론에서 말했듯이 결측치가 아닌 부분을 통해서만 학습을 진행하려는 의도입니다. 이후에 해당 부분을 통해서 모든 평점 부분에 대해서 gradient_descent를 실행해줍니다.","metadata":{}},{"cell_type":"code","source":"for i, j in zip(xi, yi):\n    self.gradient_descent(i, j, self._R[i, j])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_descent(self, i, j, rating):\n    \"\"\"\n    graident descent function\n\n    :param i: user index of matrix\n    :param j: item index of matrix\n    :param rating: rating of (i,j)\n    \"\"\"\n\n    # get error\n    prediction = self.get_prediction(i, j)\n    error = rating - prediction\n\n    # update biases\n    self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n    self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n    # update latent feature\n    dp, dq = self.gradient(error, i, j)\n    self._P[i, :] += self._learning_rate * dp\n    self._Q[j, :] += self._learning_rate * dq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- gradient_descent 함수의 경우 행렬의 원소 위치(i, j)와 평점 값(self._R[i, j])을 받습니다. 바로 시작하는 prediction = self.get_prediction(i, j) 은 User Latent Matrix와 Item Latent Matrix의 곱을 통해서 평점 행렬의 값들을 생성하는 부분입니다.\n\n\n- self._P[i, :].dot(self._Q[j, :].T) 에서 User Latent P와 Item Latent Q가 곱해져서 평점을 계산하고 Bias를 없애기 위해서 전체 평균(self._b), User의 평균 평점(self._b_P[i]), Item의 평균 평점(self._b_Q[j])을 더해줌으로써 값을 생성합니다.","metadata":{}},{"cell_type":"code","source":"def get_prediction(self, i, j):\n    \"\"\"\n    get predicted rating: user_i, item_j\n    :return: prediction of r_ij\n    \"\"\"\n    return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이후에, error = rating - prediction 을 통해서 얼마만큼의 차이가 있는지 계산을 합니다. 그리고 아래의 수식에서 사용한 공식을 이용해서 self.gradient(error, i, j) 에서 Gradient 값을 계산하고 Weight 및 Bais를 업데이트합니다.","metadata":{}},{"cell_type":"markdown","source":"![](https://drive.google.com/uc?export=view&id=1Nlhx0ilvzD4Vhxnj4WPrhnJNRFkRfBPf)","metadata":{}},{"cell_type":"code","source":"# update biases\nself._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\nself._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n# update latent feature\ndp, dq = self.gradient(error, i, j)\nself._P[i, :] += self._learning_rate * dp\nself._Q[j, :] += self._learning_rate * dq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이후에, cost = self.cost() 에서 전체 Matrix에 대해서 오차를 계산하고 출력해줍니다. cost += pow(self._R[x, y] - self.get_prediction(x, y), 2) 에서 각 평점 별로 오차를 계산하게 됩니다.","metadata":{}},{"cell_type":"code","source":"def cost(self):\n    \"\"\"\n    compute root mean square error\n    :return: rmse cost\n    \"\"\"\n\n    # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n    # 참고: http://codepractice.tistory.com/90\n    xi, yi = self._R.nonzero()\n    # predicted = self.get_complete_matrix()\n    cost = 0\n    for x, y in zip(xi, yi):\n        cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n    return np.sqrt(cost/len(xi))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이후, 모든 Epoch에 대해서 학습이 완료되면 factorizer.get_complete_matrix() 를 통해서 완성된 평점 행렬을 추출하게 되면 SGD를 이용한 협업 필터링이 완료되게 됩니다.","metadata":{}},{"cell_type":"code","source":"factorizer.get_complete_matrix()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"참고로 이를 조금 수정해서 PyTorch를 이용한 코드는 다음의 링크에서 확인할 수 있습니다. 다음 포스팅에서는 ALS가 무엇인지에서부터 어떻게 사용하는지에 대해 알아보도록 하겠습니다.","metadata":{}}]}